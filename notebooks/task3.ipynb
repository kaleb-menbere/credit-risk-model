{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a5f7926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TASK 3 - FEATURE ENGINEERING\n",
      "Credit Risk Model using Alternative Data\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "1. LOADING DATA\n",
      "======================================================================\n",
      "File not found at data/raw/data.csv\n",
      "Trying alternative path: ../data/raw/data.csv\n",
      "âœ“ Successfully loaded 50,000 rows from ../data/raw/data.csv\n",
      "\n",
      "Data loaded successfully!\n",
      "Shape: (50000, 16)\n",
      "Columns: TransactionId, BatchId, AccountId, SubscriptionId, CustomerId, CurrencyCode, CountryCode, ProviderId, ProductId, ProductCategory, ChannelId, Amount, Value, TransactionStartTime, PricingStrategy, FraudResult\n",
      "\n",
      "======================================================================\n",
      "2. DATA CLEANING\n",
      "======================================================================\n",
      "âœ“ Converted TransactionStartTime to datetime\n",
      "âœ“ Converted Amount to numeric\n",
      "âœ“ Converted Value to numeric\n",
      "âœ“ Converted BatchId to numeric\n",
      "\n",
      "Missing values summary:\n",
      "  BatchId: 50,000 missing (100.0%)\n",
      "\n",
      "======================================================================\n",
      "3. CREATING RFMS FEATURES\n",
      "======================================================================\n",
      "Extracting time-based features...\n",
      "âœ“ Extracted hour, day, month, year, dayofweek, weekend indicators\n",
      "Snapshot date for recency: 2019-01-03 19:41:16+00:00\n",
      "\n",
      "======================================================================\n",
      "4. CUSTOMER-LEVEL AGGREGATION\n",
      "======================================================================\n",
      "Grouping transactions by customer...\n",
      "âœ“ Created features for 2,293 unique customers\n",
      "\n",
      "======================================================================\n",
      "5. DERIVED FEATURES CALCULATION\n",
      "======================================================================\n",
      "Calculating recency...\n",
      "âœ“ Calculated recency_days\n",
      "Calculating customer tenure...\n",
      "âœ“ Calculated customer_tenure_days\n",
      "Calculating frequency...\n",
      "âœ“ Calculated frequency_per_day\n",
      "Calculating monetary metrics...\n",
      "âœ“ Created 19 features per customer\n",
      "\n",
      "======================================================================\n",
      "6. FEATURE PREPARATION\n",
      "======================================================================\n",
      "Selected 11 numerical features:\n",
      "  - total_transaction_amount\n",
      "  - avg_transaction_amount\n",
      "  - std_transaction_amount\n",
      "  - transaction_count\n",
      "  - recency_days\n",
      "  - customer_tenure_days\n",
      "  - frequency_per_day\n",
      "  - avg_transaction_size\n",
      "  - transaction_range\n",
      "  - min_transaction_amount\n",
      "  - max_transaction_amount\n",
      "\n",
      "Selected 4 categorical features:\n",
      "  - most_common_productcategory\n",
      "  - most_common_channelid\n",
      "  - most_common_currencycode\n",
      "  - most_common_countrycode\n",
      "\n",
      "======================================================================\n",
      "7. NUMERICAL FEATURE PROCESSING\n",
      "======================================================================\n",
      "Processing numerical features...\n",
      "  Imputing missing values...\n",
      "  Scaling features...\n",
      "âœ“ Processed 11 numerical features\n",
      "\n",
      "======================================================================\n",
      "8. CATEGORICAL FEATURE PROCESSING\n",
      "======================================================================\n",
      "Processing 4 categorical features...\n",
      "  Applying one-hot encoding...\n",
      "âœ“ Created 12 encoded categorical features\n",
      "\n",
      "âœ“ Final feature matrix: 2293 customers Ã— 23 features\n",
      "\n",
      "======================================================================\n",
      "9. SAVING RESULTS\n",
      "======================================================================\n",
      "âœ“ Saved customer features to data/processed/customer_features.csv\n",
      "âœ“ Saved processed feature matrix to data/processed/X_processed.csv\n",
      "âœ“ Saved feature names to data/processed/feature_names.txt\n",
      "âœ“ Saved metadata to data/processed/feature_metadata.json\n",
      "\n",
      "======================================================================\n",
      "FEATURE ENGINEERING COMPLETE - SUMMARY\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š DATA OVERVIEW\n",
      "   Original transactions: 50,000\n",
      "   Unique customers: 2,293\n",
      "   Date range: 2018-11-15 to 2019-01-03\n",
      "\n",
      "ðŸŽ¯ RFMS FEATURES CREATED\n",
      "   Recency (R): Days since last transaction\n",
      "   Frequency (F): Transactions per day\n",
      "   Monetary (M): Total and average transaction amount\n",
      "   Standard Deviation (S): Transaction amount variability\n",
      "\n",
      "ðŸ”¢ FEATURE STATISTICS\n",
      "   Total features: 23\n",
      "   Numerical features: 11\n",
      "   Categorical features (encoded): 12\n",
      "\n",
      "ðŸ’¾ FILES SAVED\n",
      "   1. data/processed/customer_features.csv - Raw customer-level features\n",
      "   2. data/processed/X_processed.csv - Processed feature matrix for modeling\n",
      "   3. data/processed/feature_names.txt - List of all feature names\n",
      "   4. data/processed/feature_metadata.json - Processing metadata\n",
      "\n",
      "ðŸ“ˆ SAMPLE OF PROCESSED FEATURES (first 5 customers):\n",
      "  total_transaction_amount avg_transaction_amount std_transaction_amount transaction_count recency_days customer_tenure_days frequency_per_day avg_transaction_size transaction_range min_transaction_amount max_transaction_amount most_common_countrycode most_common_productcategory_data_bundles most_common_productcategory_financial_services most_common_productcategory_movies most_common_productcategory_other most_common_productcategory_ticket most_common_productcategory_transport most_common_productcategory_tv most_common_productcategory_utility_bill most_common_channelid_ChannelId_2 most_common_channelid_ChannelId_3 most_common_channelid_ChannelId_5\n",
      "0                -0.072855              -0.293437              -0.166752         -0.261724     1.809814            -0.708994         -0.315297            -0.293437         -0.158882               -0.21468              -0.213632                     256                                    False                                          False                              False                             False                              False                                 False                          False                                    False                              True                             False                             False\n",
      "1                -0.072855              -0.293437              -0.166752         -0.261724     1.809814            -0.708994         -0.315297            -0.293437         -0.158882               -0.21468              -0.213632                     256                                    False                                          False                              False                             False                              False                                 False                          False                                    False                              True                             False                             False\n",
      "2                -0.055681              -0.110294              -0.063357         -0.211406     2.153566            -0.708994          0.283876            -0.110294         -0.089723              -0.136688               -0.12503                     256                                    False                                           True                              False                             False                              False                                 False                          False                                    False                             False                              True                             False\n",
      "3                -0.065227              -0.157183              -0.157565         -0.173667     0.503559              0.86651         -0.415159            -0.157183          -0.15162              -0.059866              -0.162686                     256                                    False                                          False                              False                             False                              False                                 False                          False                                    False                              True                             False                             False\n",
      "4                -0.065985              -0.136457              -0.166752         -0.261724    -0.321444            -0.708994         -0.315297            -0.136457         -0.158882              -0.027499              -0.160471                     256                                    False                                          False                              False                             False                              False                                 False                          False                                    False                             False                              True                             False\n",
      "\n",
      "======================================================================\n",
      "NEXT STEP: Task 4 - Proxy Target Variable Engineering\n",
      "======================================================================\n",
      "\n",
      "Ready for RFM clustering to create proxy risk labels!\n",
      "Use customer_features.csv as input for K-Means clustering.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 3 - Feature Engineering for Credit Risk Model\n",
    "Complete self-contained script with error handling\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TASK 3 - FEATURE ENGINEERING\")\n",
    "print(\"Credit Risk Model using Alternative Data\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def create_sample_data(n_transactions=10000, n_customers=200):\n",
    "    \"\"\"Create synthetic transaction data for testing\"\"\"\n",
    "    print(\"Creating sample transaction data for testing...\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate customer IDs\n",
    "    customer_ids = [f'CUST{str(i).zfill(5)}' for i in range(n_customers)]\n",
    "    \n",
    "    # Generate dates\n",
    "    start_date = pd.Timestamp('2023-01-01')\n",
    "    end_date = pd.Timestamp('2024-01-01')\n",
    "    dates = pd.date_range(start=start_date, end=end_date, periods=n_transactions)\n",
    "    \n",
    "    # Create dataframe\n",
    "    data = {\n",
    "        'TransactionId': [f'TX{str(i).zfill(7)}' for i in range(n_transactions)],\n",
    "        'CustomerId': np.random.choice(customer_ids, n_transactions, p=np.random.dirichlet(np.ones(n_customers))),\n",
    "        'TransactionStartTime': np.random.choice(dates, n_transactions),\n",
    "        'Amount': np.random.exponential(5000, n_transactions) * np.random.choice([1, -1], n_transactions, p=[0.95, 0.05]),\n",
    "        'Value': np.abs(np.random.exponential(5000, n_transactions)),\n",
    "        'ProductCategory': np.random.choice(['Communications', 'Groceries', 'Entertainment', 'Transport', 'Electronics'], \n",
    "                                          n_transactions, p=[0.35, 0.25, 0.15, 0.15, 0.10]),\n",
    "        'ChannelId': np.random.choice(['Android', 'Web', 'iOS', 'Pay Later'], \n",
    "                                     n_transactions, p=[0.45, 0.30, 0.15, 0.10]),\n",
    "        'FraudResult': np.random.binomial(1, 0.01, n_transactions),\n",
    "        'CurrencyCode': ['UGX'] * n_transactions,\n",
    "        'CountryCode': ['UG'] * n_transactions\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Created sample data with {n_transactions} transactions for {n_customers} customers\")\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load Data\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"1. LOADING DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "df = None\n",
    "data_path = \"data/raw/data.csv\"\n",
    "\n",
    "# Try multiple data loading approaches\n",
    "try:\n",
    "    # Try to load from specified path\n",
    "    if os.path.exists(data_path):\n",
    "        print(f\"Loading data from {data_path}...\")\n",
    "        # Load first 50k rows for testing if file is large\n",
    "        df = pd.read_csv(data_path, nrows=50000)\n",
    "        print(f\"âœ“ Successfully loaded {len(df):,} rows from {data_path}\")\n",
    "    else:\n",
    "        print(f\"File not found at {data_path}\")\n",
    "        # Try alternative path\n",
    "        alt_path = \"../data/raw/data.csv\"\n",
    "        if os.path.exists(alt_path):\n",
    "            print(f\"Trying alternative path: {alt_path}\")\n",
    "            df = pd.read_csv(alt_path, nrows=50000)\n",
    "            print(f\"âœ“ Successfully loaded {len(df):,} rows from {alt_path}\")\n",
    "        else:\n",
    "            print(\"No data file found. Creating sample data...\")\n",
    "            df = create_sample_data()\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Creating sample data instead...\")\n",
    "    df = create_sample_data()\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {', '.join(df.columns.tolist())}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Basic Data Cleaning\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2. DATA CLEANING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Make a copy\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Convert datetime\n",
    "if 'TransactionStartTime' in df_clean.columns:\n",
    "    df_clean['TransactionStartTime'] = pd.to_datetime(df_clean['TransactionStartTime'], errors='coerce')\n",
    "    print(f\"âœ“ Converted TransactionStartTime to datetime\")\n",
    "\n",
    "# Ensure numeric columns\n",
    "numeric_cols = ['Amount', 'Value', 'BatchId'] if 'BatchId' in df_clean.columns else ['Amount', 'Value']\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "        print(f\"âœ“ Converted {col} to numeric\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values summary:\")\n",
    "missing = df_clean.isnull().sum()\n",
    "for col in missing[missing > 0].index:\n",
    "    print(f\"  {col}: {missing[col]:,} missing ({missing[col]/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Create RFMS Features\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3. CREATING RFMS FEATURES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract time features from transaction timestamp\n",
    "if 'TransactionStartTime' in df_clean.columns:\n",
    "    print(\"Extracting time-based features...\")\n",
    "    df_clean['transaction_hour'] = df_clean['TransactionStartTime'].dt.hour\n",
    "    df_clean['transaction_day'] = df_clean['TransactionStartTime'].dt.day\n",
    "    df_clean['transaction_month'] = df_clean['TransactionStartTime'].dt.month\n",
    "    df_clean['transaction_year'] = df_clean['TransactionStartTime'].dt.year\n",
    "    df_clean['transaction_dayofweek'] = df_clean['TransactionStartTime'].dt.dayofweek\n",
    "    df_clean['transaction_weekend'] = df_clean['TransactionStartTime'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    print(\"âœ“ Extracted hour, day, month, year, dayofweek, weekend indicators\")\n",
    "\n",
    "# Define snapshot date for recency calculation\n",
    "snapshot_date = df_clean['TransactionStartTime'].max() if 'TransactionStartTime' in df_clean.columns else pd.Timestamp.now()\n",
    "print(f\"Snapshot date for recency: {snapshot_date}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Customer-Level Aggregation\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4. CUSTOMER-LEVEL AGGREGATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Grouping transactions by customer...\")\n",
    "\n",
    "# Define aggregation functions\n",
    "agg_functions = {\n",
    "    'Amount': ['sum', 'mean', 'std', 'min', 'max', 'count'],\n",
    "    'Value': ['mean', 'std']\n",
    "}\n",
    "\n",
    "# Add datetime aggregations if available\n",
    "if 'TransactionStartTime' in df_clean.columns:\n",
    "    agg_functions['TransactionStartTime'] = ['min', 'max']\n",
    "\n",
    "# Add categorical aggregations\n",
    "categorical_cols = ['ProductCategory', 'ChannelId', 'CurrencyCode', 'CountryCode']\n",
    "for col in categorical_cols:\n",
    "    if col in df_clean.columns:\n",
    "        agg_functions[col] = lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'\n",
    "\n",
    "# Perform aggregation\n",
    "customer_features = df_clean.groupby('CustomerId').agg(agg_functions)\n",
    "\n",
    "# Flatten column names\n",
    "customer_features.columns = ['_'.join(col).strip() for col in customer_features.columns.values]\n",
    "\n",
    "# Rename key columns for clarity\n",
    "column_rename = {\n",
    "    'Amount_sum': 'total_transaction_amount',\n",
    "    'Amount_mean': 'avg_transaction_amount',\n",
    "    'Amount_std': 'std_transaction_amount',\n",
    "    'Amount_min': 'min_transaction_amount',\n",
    "    'Amount_max': 'max_transaction_amount',\n",
    "    'Amount_count': 'transaction_count',\n",
    "    'Value_mean': 'avg_transaction_value',\n",
    "    'Value_std': 'std_transaction_value'\n",
    "}\n",
    "\n",
    "if 'TransactionStartTime_min' in customer_features.columns:\n",
    "    column_rename['TransactionStartTime_min'] = 'first_transaction_date'\n",
    "    column_rename['TransactionStartTime_max'] = 'last_transaction_date'\n",
    "\n",
    "customer_features = customer_features.rename(columns=column_rename)\n",
    "\n",
    "# Reset index\n",
    "customer_features = customer_features.reset_index()\n",
    "\n",
    "print(f\"âœ“ Created features for {len(customer_features):,} unique customers\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Calculate Derived RFMS Features\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"5. DERIVED FEATURES CALCULATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Calculate recency (days since last transaction)\n",
    "if 'last_transaction_date' in customer_features.columns:\n",
    "    print(\"Calculating recency...\")\n",
    "    customer_features['recency_days'] = (snapshot_date - customer_features['last_transaction_date']).dt.days\n",
    "    print(\"âœ“ Calculated recency_days\")\n",
    "\n",
    "# Calculate customer tenure\n",
    "if all(col in customer_features.columns for col in ['last_transaction_date', 'first_transaction_date']):\n",
    "    print(\"Calculating customer tenure...\")\n",
    "    customer_features['customer_tenure_days'] = (\n",
    "        customer_features['last_transaction_date'] - customer_features['first_transaction_date']\n",
    "    ).dt.days\n",
    "    print(\"âœ“ Calculated customer_tenure_days\")\n",
    "\n",
    "# Calculate frequency (transactions per day)\n",
    "if all(col in customer_features.columns for col in ['transaction_count', 'customer_tenure_days']):\n",
    "    print(\"Calculating frequency...\")\n",
    "    customer_features['frequency_per_day'] = (\n",
    "        customer_features['transaction_count'] / \n",
    "        np.maximum(customer_features['customer_tenure_days'], 1)\n",
    "    )\n",
    "    print(\"âœ“ Calculated frequency_per_day\")\n",
    "\n",
    "# Calculate monetary metrics\n",
    "print(\"Calculating monetary metrics...\")\n",
    "if 'total_transaction_amount' in customer_features.columns and 'transaction_count' in customer_features.columns:\n",
    "    customer_features['avg_transaction_size'] = (\n",
    "        customer_features['total_transaction_amount'] / customer_features['transaction_count']\n",
    "    )\n",
    "\n",
    "# Handle NaN values in standard deviation (customers with only 1 transaction)\n",
    "if 'std_transaction_amount' in customer_features.columns:\n",
    "    customer_features['std_transaction_amount'] = customer_features['std_transaction_amount'].fillna(0)\n",
    "\n",
    "# Calculate transaction amount range\n",
    "if all(col in customer_features.columns for col in ['max_transaction_amount', 'min_transaction_amount']):\n",
    "    customer_features['transaction_range'] = (\n",
    "        customer_features['max_transaction_amount'] - customer_features['min_transaction_amount']\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ Created {len(customer_features.columns) - 1} features per customer\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Feature Selection and Preparation\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"6. FEATURE PREPARATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define core RFMS features\n",
    "core_numerical_features = [\n",
    "    'total_transaction_amount',\n",
    "    'avg_transaction_amount',\n",
    "    'std_transaction_amount',\n",
    "    'transaction_count',\n",
    "    'recency_days',\n",
    "    'customer_tenure_days',\n",
    "    'frequency_per_day'\n",
    "]\n",
    "\n",
    "# Add optional features if they exist\n",
    "optional_features = ['avg_transaction_size', 'transaction_range', 'min_transaction_amount', 'max_transaction_amount']\n",
    "for feat in optional_features:\n",
    "    if feat in customer_features.columns:\n",
    "        core_numerical_features.append(feat)\n",
    "\n",
    "# Select only features that exist\n",
    "numerical_features = [f for f in core_numerical_features if f in customer_features.columns]\n",
    "\n",
    "# Define categorical features\n",
    "categorical_features = []\n",
    "for col in ['ProductCategory', 'ChannelId', 'CurrencyCode', 'CountryCode']:\n",
    "    if f'{col}_<lambda>' in customer_features.columns:\n",
    "        categorical_features.append(f'{col}_<lambda>')\n",
    "        # Rename for clarity\n",
    "        new_name = f'most_common_{col.lower()}'\n",
    "        customer_features = customer_features.rename(columns={f'{col}_<lambda>': new_name})\n",
    "        categorical_features[-1] = new_name\n",
    "\n",
    "print(f\"Selected {len(numerical_features)} numerical features:\")\n",
    "for feat in numerical_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "print(f\"\\nSelected {len(categorical_features)} categorical features:\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Process Numerical Features\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"7. NUMERICAL FEATURE PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Processing numerical features...\")\n",
    "\n",
    "# Separate numerical features\n",
    "X_num = customer_features[numerical_features].copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(\"  Imputing missing values...\")\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_num_imputed = imputer.fit_transform(X_num)\n",
    "\n",
    "# Scale features\n",
    "print(\"  Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X_num_scaled = scaler.fit_transform(X_num_imputed)\n",
    "\n",
    "print(f\"âœ“ Processed {X_num_scaled.shape[1]} numerical features\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Process Categorical Features\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"8. CATEGORICAL FEATURE PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if categorical_features:\n",
    "    print(f\"Processing {len(categorical_features)} categorical features...\")\n",
    "    \n",
    "    # Extract categorical data\n",
    "    X_cat = customer_features[categorical_features].copy()\n",
    "    \n",
    "    # One-hot encoding\n",
    "    print(\"  Applying one-hot encoding...\")\n",
    "    X_cat_encoded = pd.get_dummies(X_cat, drop_first=True, prefix_sep='_')\n",
    "    \n",
    "    # Combine with numerical features\n",
    "    X_processed = np.hstack([X_num_scaled, X_cat_encoded])\n",
    "    feature_names = numerical_features + X_cat_encoded.columns.tolist()\n",
    "    \n",
    "    print(f\"âœ“ Created {X_cat_encoded.shape[1]} encoded categorical features\")\n",
    "else:\n",
    "    print(\"No categorical features to process\")\n",
    "    X_processed = X_num_scaled\n",
    "    feature_names = numerical_features\n",
    "\n",
    "print(f\"\\nâœ“ Final feature matrix: {X_processed.shape[0]} customers Ã— {X_processed.shape[1]} features\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Save Results\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"9. SAVING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "# Save customer features\n",
    "customer_features_path = 'data/processed/customer_features.csv'\n",
    "customer_features.to_csv(customer_features_path, index=False)\n",
    "print(f\"âœ“ Saved customer features to {customer_features_path}\")\n",
    "\n",
    "# Save processed feature matrix\n",
    "processed_path = 'data/processed/X_processed.csv'\n",
    "processed_df = pd.DataFrame(X_processed, columns=feature_names)\n",
    "processed_df.to_csv(processed_path, index=False)\n",
    "print(f\"âœ“ Saved processed feature matrix to {processed_path}\")\n",
    "\n",
    "# Save feature names separately\n",
    "feature_names_path = 'data/processed/feature_names.txt'\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(name + '\\n')\n",
    "print(f\"âœ“ Saved feature names to {feature_names_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'task': 'Task 3 - Feature Engineering',\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'data_source': 'Xente eCommerce Transactions',\n",
    "    'n_customers': len(customer_features),\n",
    "    'n_transactions_original': len(df),\n",
    "    'n_features_total': X_processed.shape[1],\n",
    "    'n_numerical_features': len(numerical_features),\n",
    "    'n_categorical_features': len(categorical_features),\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'rfms_features_created': ['Recency', 'Frequency', 'Monetary', 'Standard Deviation'],\n",
    "    'processing_steps': [\n",
    "        'Datetime feature extraction',\n",
    "        'Customer-level aggregation',\n",
    "        'RFMS calculation',\n",
    "        'Missing value imputation',\n",
    "        'Feature scaling',\n",
    "        'One-hot encoding'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_path = 'data/processed/feature_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"âœ“ Saved metadata to {metadata_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Summary Report\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATA OVERVIEW\")\n",
    "print(f\"   Original transactions: {len(df):,}\")\n",
    "print(f\"   Unique customers: {len(customer_features):,}\")\n",
    "print(f\"   Date range: {df_clean['TransactionStartTime'].min().date() if 'TransactionStartTime' in df_clean.columns else 'N/A'} to {df_clean['TransactionStartTime'].max().date() if 'TransactionStartTime' in df_clean.columns else 'N/A'}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ RFMS FEATURES CREATED\")\n",
    "print(f\"   Recency (R): Days since last transaction\")\n",
    "print(f\"   Frequency (F): Transactions per day\")\n",
    "print(f\"   Monetary (M): Total and average transaction amount\")\n",
    "print(f\"   Standard Deviation (S): Transaction amount variability\")\n",
    "\n",
    "print(f\"\\nðŸ”¢ FEATURE STATISTICS\")\n",
    "print(f\"   Total features: {X_processed.shape[1]}\")\n",
    "print(f\"   Numerical features: {len(numerical_features)}\")\n",
    "print(f\"   Categorical features (encoded): {X_processed.shape[1] - len(numerical_features)}\")\n",
    "\n",
    "print(f\"\\nðŸ’¾ FILES SAVED\")\n",
    "print(f\"   1. data/processed/customer_features.csv - Raw customer-level features\")\n",
    "print(f\"   2. data/processed/X_processed.csv - Processed feature matrix for modeling\")\n",
    "print(f\"   3. data/processed/feature_names.txt - List of all feature names\")\n",
    "print(f\"   4. data/processed/feature_metadata.json - Processing metadata\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ SAMPLE OF PROCESSED FEATURES (first 5 customers):\")\n",
    "print(processed_df.head().to_string())\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT STEP: Task 4 - Proxy Target Variable Engineering\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nReady for RFM clustering to create proxy risk labels!\")\n",
    "print(\"Use customer_features.csv as input for K-Means clustering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c271b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
