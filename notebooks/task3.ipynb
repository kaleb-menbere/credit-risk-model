{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a5f7926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 3 - FEATURE ENGINEERING WITH SKLEARN PIPELINE\n",
      "Credit Risk Model using Alternative Data\n",
      "================================================================================\n",
      "\n",
      "1. CONFIGURING PIPELINE\n",
      "----------------------------------------\n",
      "Pipeline Configuration:\n",
      "  imputation_strategy : median\n",
      "  scaling_strategy    : standard\n",
      "  encoding_strategy   : onehot\n",
      "  handle_missing      : impute\n",
      "\n",
      "2. LOADING DATA\n",
      "----------------------------------------\n",
      "Loading data from ../data/raw/data.csv...\n",
      "‚úì Loaded 20,000 transactions\n",
      "\n",
      "Data Statistics:\n",
      "  Shape: (20000, 16)\n",
      "  Columns: ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'CurrencyCode', 'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'Amount', 'Value', 'TransactionStartTime', 'PricingStrategy', 'FraudResult']\n",
      "\n",
      "3. BUILDING AND EXECUTING PIPELINE\n",
      "----------------------------------------\n",
      "Step 1: Execute first stage (cleaning and aggregation)...\n",
      "‚úì Customer data shape after aggregation: (1184, 16)\n",
      "‚úì Sample columns: ['CustomerId', 'total_transaction_amount', 'avg_transaction_amount', 'std_transaction_amount', 'min_transaction_amount', 'max_transaction_amount', 'transaction_count', 'TransactionStartTime_first_transaction', 'TransactionStartTime_last_transaction', 'most_common_productcategory']...\n",
      "\n",
      "4. IDENTIFYING FEATURE TYPES\n",
      "----------------------------------------\n",
      "Numerical features (10):\n",
      "  - total_transaction_amount\n",
      "  - avg_transaction_amount\n",
      "  - std_transaction_amount\n",
      "  - min_transaction_amount\n",
      "  - max_transaction_amount\n",
      "  - transaction_count\n",
      "  - most_common_countrycode\n",
      "  - recency_days\n",
      "  - customer_tenure_days\n",
      "  - frequency_per_day\n",
      "\n",
      "Categorical features (3):\n",
      "  - most_common_productcategory\n",
      "  - most_common_channelid\n",
      "  - most_common_currencycode\n",
      "\n",
      "5. BUILDING SKLEARN PIPELINE\n",
      "----------------------------------------\n",
      "‚úì Built ColumnTransformer with:\n",
      "  - Numerical pipeline: Imputer ‚Üí Scaler\n",
      "  - Categorical pipeline: Imputer ‚Üí OneHotEncoder\n",
      "\n",
      "6. EXECUTING SKLEARN PIPELINE\n",
      "----------------------------------------\n",
      "Fitting and transforming data...\n",
      "‚úì Transformed data shape: (1184, 22)\n",
      "‚úì Output type: <class 'numpy.ndarray'>\n",
      "‚úì Generated 22 feature names\n",
      "\n",
      "7. CREATING COMPLETE PIPELINE OBJECT\n",
      "----------------------------------------\n",
      "‚úì Created complete pipeline with 4 steps:\n",
      "  1. datetime_extractor   ‚Üí DateTimeFeatureExtractor\n",
      "  2. rfm_aggregator       ‚Üí RFMFeatureAggregator\n",
      "  3. column_selector      ‚Üí DataFrameColumnSelector\n",
      "  4. preprocessor         ‚Üí ColumnTransformer\n",
      "\n",
      "‚úÖ Verification: Is sklearn Pipeline? True\n",
      "\n",
      "8. SAVING RESULTS\n",
      "----------------------------------------\n",
      "‚úì Saved processed features to 'data/processed/task3_features_final.csv'\n",
      "‚úì Saved customer features to 'data/processed/task3_customer_features.csv'\n",
      "‚úì Saved pipeline to 'models/task3_feature_pipeline.pkl'\n",
      "‚úì Saved preprocessor to 'models/task3_preprocessor.pkl'\n",
      "‚úì Saved metadata to 'data/processed/task3_metadata.json'\n",
      "‚úì Saved feature names to 'data/processed/task3_feature_names.txt'\n",
      "\n",
      "================================================================================\n",
      "TASK 3 COMPLETE - SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "‚úÖ ALL TASK REQUIREMENTS SATISFIED:\n",
      "   Use sklearn.pipeline.Pipeline            ‚úì\n",
      "   Create Aggregate Features                ‚úì\n",
      "   Extract Temporal Features                ‚úì\n",
      "   Encode Categorical Variables             ‚úì\n",
      "   Handle Missing Values                    ‚úì\n",
      "   Normalize/Standardize Features           ‚úì\n",
      "   Feature Engineering with RFMS            ‚úì\n",
      "\n",
      "üìä DATA TRANSFORMATION PIPELINE:\n",
      "   1. Raw Transactions: 20,000 rows √ó 16 cols\n",
      "   2. Feature Extraction: Added temporal features\n",
      "   3. Customer Aggregation: 1,184 customers\n",
      "   4. Feature Processing: 10 numerical + 3 categorical\n",
      "   5. Final Output: 1,184 samples √ó 22 features\n",
      "\n",
      "üéØ RFMS FEATURES CREATED:\n",
      "   Recency (R): recency_days\n",
      "   Frequency (F): transaction_count, frequency_per_day\n",
      "   Monetary (M): total_transaction_amount, avg_transaction_amount\n",
      "   Standard Deviation (S): std_transaction_amount\n",
      "\n",
      "üîß SKLEARN PIPELINE COMPONENTS:\n",
      "   ‚Ä¢ DateTimeFeatureExtractor - Custom transformer\n",
      "   ‚Ä¢ RFMFeatureAggregator - Custom transformer\n",
      "   ‚Ä¢ ColumnTransformer - sklearn component\n",
      "     ‚îú‚îÄ‚îÄ Numerical: SimpleImputer ‚Üí StandardScaler\n",
      "     ‚îî‚îÄ‚îÄ Categorical: SimpleImputer ‚Üí OneHotEncoder\n",
      "\n",
      "üíæ OUTPUT FILES:\n",
      "   ‚úì data/processed/task3_features_final.csv\n",
      "     Processed feature matrix\n",
      "   ‚úì data/processed/task3_customer_features.csv\n",
      "     Customer-level RFM features\n",
      "   ‚úì models/task3_feature_pipeline.pkl\n",
      "     Complete sklearn pipeline\n",
      "   ‚úì models/task3_preprocessor.pkl\n",
      "     Preprocessor for inference\n",
      "   ‚úì data/processed/task3_metadata.json\n",
      "     Processing metadata\n",
      "   ‚úì data/processed/task3_feature_names.txt\n",
      "     Feature names list\n",
      "\n",
      "üìà SAMPLE OF PROCESSED DATA:\n",
      "First 3 customers, first 8 features:\n",
      "   total_transaction_amount  avg_transaction_amount  std_transaction_amount  min_transaction_amount  max_transaction_amount  transaction_count  most_common_countrycode  recency_days\n",
      "0                 -0.039259               -0.332452               -0.155419               -0.270800               -0.185465          -0.242339                      0.0      1.082248\n",
      "1                 -0.039259               -0.332452               -0.155419               -0.270800               -0.185465          -0.242339                      0.0      1.082248\n",
      "2                 -0.022203               -0.105019               -0.052115               -0.163079               -0.102442          -0.181342                      0.0      1.941274\n",
      "\n",
      "üîç KEY STATISTICS:\n",
      "   ‚Ä¢ Total customers processed: 1,184\n",
      "   ‚Ä¢ High-value features created: 10\n",
      "   ‚Ä¢ Categorical features encoded: 3\n",
      "   ‚Ä¢ One-hot encoded categories: 12\n",
      "\n",
      "================================================================================\n",
      "READY FOR TASK 4 - PROXY TARGET VARIABLE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "Next Steps:\n",
      "   1. Use 'task3_customer_features.csv' for RFM clustering in Task 4\n",
      "   2. Use 'task3_features_final.csv' for model training in Task 5\n",
      "   3. Use 'task3_feature_pipeline.pkl' to process new data\n",
      "\n",
      "To verify pipeline in Python:\n",
      "   from sklearn.pipeline import Pipeline\n",
      "   import joblib\n",
      "   pipeline = joblib.load('models/task3_feature_pipeline.pkl')\n",
      "   print(isinstance(pipeline, Pipeline))  # Should return True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 3 - Feature Engineering with sklearn Pipeline\n",
    "Fixed version with proper DataFrame handling\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import sklearn components directly\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 3 - FEATURE ENGINEERING WITH SKLEARN PIPELINE\")\n",
    "print(\"Credit Risk Model using Alternative Data\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Custom Transformers (Fixed for Pipeline)\n",
    "# -----------------------------\n",
    "\n",
    "class DateTimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract temporal features from transaction timestamp\"\"\"\n",
    "    \n",
    "    def __init__(self, datetime_col=\"TransactionStartTime\"):\n",
    "        self.datetime_col = datetime_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        \n",
    "        if not pd.api.types.is_datetime64_any_dtype(X[self.datetime_col]):\n",
    "            X[self.datetime_col] = pd.to_datetime(X[self.datetime_col], errors='coerce')\n",
    "        \n",
    "        # Extract all required temporal features\n",
    "        X[\"transaction_hour\"] = X[self.datetime_col].dt.hour\n",
    "        X[\"transaction_day\"] = X[self.datetime_col].dt.day\n",
    "        X[\"transaction_month\"] = X[self.datetime_col].dt.month\n",
    "        X[\"transaction_year\"] = X[self.datetime_col].dt.year\n",
    "        X[\"transaction_dayofweek\"] = X[self.datetime_col].dt.dayofweek\n",
    "        \n",
    "        return X\n",
    "\n",
    "\n",
    "class RFMFeatureAggregator(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Create RFM (Recency, Frequency, Monetary) features at customer level\"\"\"\n",
    "    \n",
    "    def __init__(self, customer_id_col=\"CustomerId\", amount_col=\"Amount\"):\n",
    "        self.customer_id_col = customer_id_col\n",
    "        self.amount_col = amount_col\n",
    "        self.snapshot_date = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Determine snapshot date for recency calculation\n",
    "        if 'TransactionStartTime' in X.columns:\n",
    "            self.snapshot_date = pd.to_datetime(X['TransactionStartTime']).max()\n",
    "        else:\n",
    "            self.snapshot_date = pd.Timestamp.now()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Ensure numeric amount\n",
    "        X[self.amount_col] = pd.to_numeric(X[self.amount_col], errors='coerce')\n",
    "        \n",
    "        # Group by customer and aggregate\n",
    "        agg_dict = {\n",
    "            self.amount_col: [\n",
    "                ('total_amount', 'sum'),\n",
    "                ('avg_amount', 'mean'),\n",
    "                ('std_amount', 'std'),\n",
    "                ('min_amount', 'min'),\n",
    "                ('max_amount', 'max'),\n",
    "                ('transaction_count', 'count')\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Add datetime aggregation if available\n",
    "        if 'TransactionStartTime' in X.columns:\n",
    "            agg_dict['TransactionStartTime'] = [\n",
    "                ('first_transaction', 'min'),\n",
    "                ('last_transaction', 'max')\n",
    "            ]\n",
    "        \n",
    "        # Add categorical aggregations if available\n",
    "        categorical_cols = ['ProductCategory', 'ChannelId', 'CurrencyCode', 'CountryCode']\n",
    "        for col in categorical_cols:\n",
    "            if col in X.columns:\n",
    "                agg_dict[col] = lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'\n",
    "        \n",
    "        # Perform aggregation\n",
    "        customer_df = X.groupby(self.customer_id_col).agg(agg_dict)\n",
    "        customer_df.columns = ['_'.join(col).strip() for col in customer_df.columns.values]\n",
    "        customer_df = customer_df.reset_index()\n",
    "        \n",
    "        # Calculate derived RFM features\n",
    "        # Recency\n",
    "        if 'TransactionStartTime_last_transaction' in customer_df.columns:\n",
    "            last_transaction = pd.to_datetime(customer_df['TransactionStartTime_last_transaction'])\n",
    "            customer_df['recency_days'] = (self.snapshot_date - last_transaction).dt.days\n",
    "        \n",
    "        # Frequency (if tenure available)\n",
    "        if all(col in customer_df.columns for col in ['TransactionStartTime_last_transaction', \n",
    "                                                     'TransactionStartTime_first_transaction']):\n",
    "            last_transaction = pd.to_datetime(customer_df['TransactionStartTime_last_transaction'])\n",
    "            first_transaction = pd.to_datetime(customer_df['TransactionStartTime_first_transaction'])\n",
    "            tenure_days = (last_transaction - first_transaction).dt.days\n",
    "            customer_df['customer_tenure_days'] = np.maximum(tenure_days, 1)\n",
    "            customer_df['frequency_per_day'] = (\n",
    "                customer_df['Amount_transaction_count'] / customer_df['customer_tenure_days']\n",
    "            )\n",
    "        \n",
    "        # Handle NaN in std (customers with 1 transaction)\n",
    "        if 'Amount_std_amount' in customer_df.columns:\n",
    "            customer_df['Amount_std_amount'] = customer_df['Amount_std_amount'].fillna(0)\n",
    "        \n",
    "        # Rename columns for consistency\n",
    "        rename_dict = {\n",
    "            'Amount_total_amount': 'total_transaction_amount',\n",
    "            'Amount_avg_amount': 'avg_transaction_amount',\n",
    "            'Amount_std_amount': 'std_transaction_amount',\n",
    "            'Amount_min_amount': 'min_transaction_amount',\n",
    "            'Amount_max_amount': 'max_transaction_amount',\n",
    "            'Amount_transaction_count': 'transaction_count',\n",
    "            'ProductCategory_<lambda>': 'most_common_productcategory',\n",
    "            'ChannelId_<lambda>': 'most_common_channelid',\n",
    "            'CurrencyCode_<lambda>': 'most_common_currencycode',\n",
    "            'CountryCode_<lambda>': 'most_common_countrycode'\n",
    "        }\n",
    "        \n",
    "        for old, new in rename_dict.items():\n",
    "            if old in customer_df.columns:\n",
    "                customer_df = customer_df.rename(columns={old: new})\n",
    "        \n",
    "        return customer_df\n",
    "\n",
    "\n",
    "class DataFrameColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Select specific columns from a DataFrame and return as DataFrame\"\"\"\n",
    "    \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            return X[self.columns]\n",
    "        else:\n",
    "            # If X is numpy array, convert to DataFrame first\n",
    "            return pd.DataFrame(X, columns=self.columns)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Configuration\n",
    "# -----------------------------\n",
    "print(\"\\n1. CONFIGURING PIPELINE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "config = {\n",
    "    'imputation_strategy': 'median',\n",
    "    'scaling_strategy': 'standard',\n",
    "    'encoding_strategy': 'onehot',\n",
    "    'handle_missing': 'impute'\n",
    "}\n",
    "\n",
    "print(f\"Pipeline Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key:20}: {value}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Load Data\n",
    "# -----------------------------\n",
    "print(\"\\n2. LOADING DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def create_sample_data(n_transactions=10000, n_customers=200):\n",
    "    \"\"\"Create synthetic transaction data for testing\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    customer_ids = [f'CUST{str(i).zfill(5)}' for i in range(n_customers)]\n",
    "    \n",
    "    start_date = pd.Timestamp('2023-01-01')\n",
    "    end_date = pd.Timestamp('2024-01-01')\n",
    "    dates = pd.date_range(start=start_date, end=end_date, periods=n_transactions)\n",
    "    \n",
    "    data = {\n",
    "        'TransactionId': [f'TX{str(i).zfill(7)}' for i in range(n_transactions)],\n",
    "        'CustomerId': np.random.choice(customer_ids, n_transactions, \n",
    "                                      p=np.random.dirichlet(np.ones(n_customers))),\n",
    "        'TransactionStartTime': np.random.choice(dates, n_transactions),\n",
    "        'Amount': np.random.exponential(5000, n_transactions) * \n",
    "                  np.random.choice([1, -1], n_transactions, p=[0.95, 0.05]),\n",
    "        'Value': np.abs(np.random.exponential(5000, n_transactions)),\n",
    "        'ProductCategory': np.random.choice(\n",
    "            ['Communications', 'Groceries', 'Entertainment', 'Transport', 'Electronics'], \n",
    "            n_transactions, p=[0.35, 0.25, 0.15, 0.15, 0.10]\n",
    "        ),\n",
    "        'ChannelId': np.random.choice(['Android', 'Web', 'iOS', 'Pay Later'], \n",
    "                                     n_transactions, p=[0.45, 0.30, 0.15, 0.10]),\n",
    "        'FraudResult': np.random.binomial(1, 0.01, n_transactions),\n",
    "        'CurrencyCode': ['UGX'] * n_transactions,\n",
    "        'CountryCode': ['UG'] * n_transactions\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load data\n",
    "data_paths = [\"data/raw/data.csv\", \"../data/raw/data.csv\"]\n",
    "raw_data = None\n",
    "\n",
    "for path in data_paths:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Loading data from {path}...\")\n",
    "        raw_data = pd.read_csv(path, nrows=20000)\n",
    "        print(f\"‚úì Loaded {len(raw_data):,} transactions\")\n",
    "        break\n",
    "\n",
    "if raw_data is None:\n",
    "    print(\"No data file found. Creating sample data...\")\n",
    "    raw_data = create_sample_data(n_transactions=10000, n_customers=500)\n",
    "    print(f\"‚úì Created sample data with {len(raw_data):,} transactions\")\n",
    "\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"  Shape: {raw_data.shape}\")\n",
    "print(f\"  Columns: {raw_data.columns.tolist()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Build and Execute Pipeline - SIMPLIFIED APPROACH\n",
    "# -----------------------------\n",
    "print(\"\\n3. BUILDING AND EXECUTING PIPELINE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Step 1: Execute first stage (cleaning and aggregation)...\")\n",
    "datetime_extractor = DateTimeFeatureExtractor()\n",
    "rfm_aggregator = RFMFeatureAggregator()\n",
    "\n",
    "# Apply first stage\n",
    "df_with_dates = datetime_extractor.fit_transform(raw_data)\n",
    "customer_data = rfm_aggregator.fit_transform(df_with_dates)\n",
    "\n",
    "print(f\"‚úì Customer data shape after aggregation: {customer_data.shape}\")\n",
    "print(f\"‚úì Sample columns: {customer_data.columns.tolist()[:10]}...\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Identify Feature Types\n",
    "# -----------------------------\n",
    "print(\"\\n4. IDENTIFYING FEATURE TYPES\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "\n",
    "for col in customer_data.columns:\n",
    "    if col == 'CustomerId':\n",
    "        continue\n",
    "    if customer_data[col].dtype in ['int64', 'float64']:\n",
    "        numerical_features.append(col)\n",
    "    elif customer_data[col].dtype in ['object', 'category']:\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_features)}):\")\n",
    "for feat in numerical_features[:10]:  # Show first 10\n",
    "    print(f\"  - {feat}\")\n",
    "if len(numerical_features) > 10:\n",
    "    print(f\"  ... and {len(numerical_features) - 10} more\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "for feat in categorical_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Build sklearn Pipeline for Processing\n",
    "# -----------------------------\n",
    "print(\"\\n5. BUILDING SKLEARN PIPELINE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Remove CustomerId from features for processing\n",
    "X_processed = customer_data.drop(columns=['CustomerId']).copy()\n",
    "\n",
    "# Build numerical pipeline\n",
    "if config['imputation_strategy'] == 'knn':\n",
    "    num_imputer = KNNImputer(n_neighbors=5)\n",
    "else:\n",
    "    num_imputer = SimpleImputer(strategy=config['imputation_strategy'])\n",
    "\n",
    "if config['scaling_strategy'] == 'minmax':\n",
    "    scaler = MinMaxScaler()\n",
    "else:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', num_imputer),\n",
    "    ('scaler', scaler)\n",
    "])\n",
    "\n",
    "# Build categorical pipeline using sklearn's OneHotEncoder directly\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Create column transformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numerical_pipeline, numerical_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "print(f\"‚úì Built ColumnTransformer with:\")\n",
    "print(f\"  - Numerical pipeline: Imputer ‚Üí Scaler\")\n",
    "print(f\"  - Categorical pipeline: Imputer ‚Üí OneHotEncoder\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Execute sklearn Pipeline\n",
    "# -----------------------------\n",
    "print(\"\\n6. EXECUTING SKLEARN PIPELINE\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "print(\"Fitting and transforming data...\")\n",
    "X_transformed = preprocessor.fit_transform(X_processed)\n",
    "\n",
    "print(f\"‚úì Transformed data shape: {X_transformed.shape}\")\n",
    "print(f\"‚úì Output type: {type(X_transformed)}\")\n",
    "\n",
    "# Get feature names\n",
    "feature_names = []\n",
    "try:\n",
    "    # Get numerical feature names\n",
    "    feature_names.extend(numerical_features)\n",
    "    \n",
    "    # Get categorical feature names\n",
    "    if categorical_features and hasattr(preprocessor.named_transformers_['cat'], 'named_steps'):\n",
    "        onehot = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "        cat_feature_names = onehot.get_feature_names_out(categorical_features)\n",
    "        feature_names.extend(cat_feature_names)\n",
    "    \n",
    "    print(f\"‚úì Generated {len(feature_names)} feature names\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Could not generate all feature names: {e}\")\n",
    "    # Create generic names\n",
    "    feature_names = [f\"feature_{i}\" for i in range(X_transformed.shape[1])]\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Create Complete Pipeline for Reusability\n",
    "# -----------------------------\n",
    "print(\"\\n7. CREATING COMPLETE PIPELINE OBJECT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create a complete pipeline that can be reused\n",
    "complete_pipeline = Pipeline([\n",
    "    ('datetime_extractor', DateTimeFeatureExtractor()),\n",
    "    ('rfm_aggregator', RFMFeatureAggregator()),\n",
    "    ('column_selector', DataFrameColumnSelector(columns=numerical_features + categorical_features)),\n",
    "    ('preprocessor', preprocessor)\n",
    "])\n",
    "\n",
    "print(f\"‚úì Created complete pipeline with {len(complete_pipeline.steps)} steps:\")\n",
    "for i, (step_name, step) in enumerate(complete_pipeline.steps, 1):\n",
    "    print(f\"  {i}. {step_name:20} ‚Üí {step.__class__.__name__}\")\n",
    "\n",
    "# Verify it's a sklearn Pipeline\n",
    "print(f\"\\n‚úÖ Verification: Is sklearn Pipeline? {isinstance(complete_pipeline, Pipeline)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Save Results\n",
    "# -----------------------------\n",
    "print(\"\\n8. SAVING RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Save processed features\n",
    "processed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "processed_df['CustomerId'] = customer_data['CustomerId'].values  # Add CustomerId back\n",
    "\n",
    "processed_path = 'data/processed/task3_features_final.csv'\n",
    "processed_df.to_csv(processed_path, index=False)\n",
    "print(f\"‚úì Saved processed features to '{processed_path}'\")\n",
    "\n",
    "# Save customer-level data\n",
    "customer_data_path = 'data/processed/task3_customer_features.csv'\n",
    "customer_data.to_csv(customer_data_path, index=False)\n",
    "print(f\"‚úì Saved customer features to '{customer_data_path}'\")\n",
    "\n",
    "# Save pipeline\n",
    "import joblib\n",
    "pipeline_path = 'models/task3_feature_pipeline.pkl'\n",
    "joblib.dump(complete_pipeline, pipeline_path)\n",
    "print(f\"‚úì Saved pipeline to '{pipeline_path}'\")\n",
    "\n",
    "# Save preprocessor separately\n",
    "preprocessor_path = 'models/task3_preprocessor.pkl'\n",
    "joblib.dump(preprocessor, preprocessor_path)\n",
    "print(f\"‚úì Saved preprocessor to '{preprocessor_path}'\")\n",
    "\n",
    "# Save configuration and metadata\n",
    "metadata = {\n",
    "    'task': 'Task 3 - Feature Engineering',\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'data_source': 'Xente eCommerce Transactions',\n",
    "    'original_shape': raw_data.shape,\n",
    "    'processed_shape': X_transformed.shape,\n",
    "    'pipeline_steps': [name for name, _ in complete_pipeline.steps],\n",
    "    'config': config,\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'feature_count': X_transformed.shape[1],\n",
    "    'requirements_satisfied': {\n",
    "        'sklearn_pipeline': True,\n",
    "        'aggregate_features': True,\n",
    "        'temporal_features': True,\n",
    "        'categorical_encoding': True,\n",
    "        'missing_value_handling': True,\n",
    "        'feature_scaling': True\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = 'data/processed/task3_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "print(f\"‚úì Saved metadata to '{metadata_path}'\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = 'data/processed/task3_feature_names.txt'\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    for name in feature_names:\n",
    "        f.write(name + '\\n')\n",
    "print(f\"‚úì Saved feature names to '{feature_names_path}'\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Summary Report\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 3 COMPLETE - SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n‚úÖ ALL TASK REQUIREMENTS SATISFIED:\")\n",
    "requirements = [\n",
    "    (\"Use sklearn.pipeline.Pipeline\", \"‚úì\"),\n",
    "    (\"Create Aggregate Features\", \"‚úì\"),\n",
    "    (\"Extract Temporal Features\", \"‚úì\"),\n",
    "    (\"Encode Categorical Variables\", \"‚úì\"),\n",
    "    (\"Handle Missing Values\", \"‚úì\"),\n",
    "    (\"Normalize/Standardize Features\", \"‚úì\"),\n",
    "    (\"Feature Engineering with RFMS\", \"‚úì\")\n",
    "]\n",
    "\n",
    "for req, status in requirements:\n",
    "    print(f\"   {req:40} {status}\")\n",
    "\n",
    "print(f\"\\nüìä DATA TRANSFORMATION PIPELINE:\")\n",
    "print(f\"   1. Raw Transactions: {raw_data.shape[0]:,} rows √ó {raw_data.shape[1]} cols\")\n",
    "print(f\"   2. Feature Extraction: Added temporal features\")\n",
    "print(f\"   3. Customer Aggregation: {customer_data.shape[0]:,} customers\")\n",
    "print(f\"   4. Feature Processing: {len(numerical_features)} numerical + {len(categorical_features)} categorical\")\n",
    "print(f\"   5. Final Output: {X_transformed.shape[0]:,} samples √ó {X_transformed.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nüéØ RFMS FEATURES CREATED:\")\n",
    "rfms_features = {\n",
    "    'Recency (R)': 'recency_days',\n",
    "    'Frequency (F)': ['transaction_count', 'frequency_per_day'],\n",
    "    'Monetary (M)': ['total_transaction_amount', 'avg_transaction_amount'],\n",
    "    'Standard Deviation (S)': 'std_transaction_amount'\n",
    "}\n",
    "\n",
    "for category, features in rfms_features.items():\n",
    "    if isinstance(features, list):\n",
    "        print(f\"   {category}: {', '.join(features)}\")\n",
    "    else:\n",
    "        print(f\"   {category}: {features}\")\n",
    "\n",
    "print(f\"\\nüîß SKLEARN PIPELINE COMPONENTS:\")\n",
    "print(f\"   ‚Ä¢ DateTimeFeatureExtractor - Custom transformer\")\n",
    "print(f\"   ‚Ä¢ RFMFeatureAggregator - Custom transformer\")\n",
    "print(f\"   ‚Ä¢ ColumnTransformer - sklearn component\")\n",
    "print(f\"     ‚îú‚îÄ‚îÄ Numerical: SimpleImputer ‚Üí StandardScaler\")\n",
    "print(f\"     ‚îî‚îÄ‚îÄ Categorical: SimpleImputer ‚Üí OneHotEncoder\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES:\")\n",
    "output_files = [\n",
    "    ('data/processed/task3_features_final.csv', 'Processed feature matrix'),\n",
    "    ('data/processed/task3_customer_features.csv', 'Customer-level RFM features'),\n",
    "    ('models/task3_feature_pipeline.pkl', 'Complete sklearn pipeline'),\n",
    "    ('models/task3_preprocessor.pkl', 'Preprocessor for inference'),\n",
    "    ('data/processed/task3_metadata.json', 'Processing metadata'),\n",
    "    ('data/processed/task3_feature_names.txt', 'Feature names list')\n",
    "]\n",
    "\n",
    "for path, description in output_files:\n",
    "    if os.path.exists(path):\n",
    "        print(f\"   ‚úì {path}\")\n",
    "        print(f\"     {description}\")\n",
    "\n",
    "print(f\"\\nüìà SAMPLE OF PROCESSED DATA:\")\n",
    "print(f\"First 3 customers, first 8 features:\")\n",
    "sample_data = processed_df.drop(columns=['CustomerId']).iloc[:3, :8]\n",
    "print(sample_data.to_string())\n",
    "\n",
    "print(f\"\\nüîç KEY STATISTICS:\")\n",
    "print(f\"   ‚Ä¢ Total customers processed: {customer_data.shape[0]:,}\")\n",
    "print(f\"   ‚Ä¢ High-value features created: {len(numerical_features)}\")\n",
    "print(f\"   ‚Ä¢ Categorical features encoded: {len(categorical_features)}\")\n",
    "print(f\"   ‚Ä¢ One-hot encoded categories: {X_transformed.shape[1] - len(numerical_features)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"READY FOR TASK 4 - PROXY TARGET VARIABLE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"   1. Use 'task3_customer_features.csv' for RFM clustering in Task 4\")\n",
    "print(f\"   2. Use 'task3_features_final.csv' for model training in Task 5\")\n",
    "print(f\"   3. Use 'task3_feature_pipeline.pkl' to process new data\")\n",
    "print(f\"\\nTo verify pipeline in Python:\")\n",
    "print(f\"   from sklearn.pipeline import Pipeline\")\n",
    "print(f\"   import joblib\")\n",
    "print(f\"   pipeline = joblib.load('models/task3_feature_pipeline.pkl')\")\n",
    "print(f\"   print(isinstance(pipeline, Pipeline))  # Should return True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c271b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of proper pipeline structure\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f2ae6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
