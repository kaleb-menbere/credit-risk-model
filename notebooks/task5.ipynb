{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a7ce66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:36:52 INFO mlflow.tracking.fluent: Experiment with name 'credit-risk-modeling' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TASK 5 - MODEL TRAINING AND TRACKING\n",
      "Credit Risk Model with MLflow Experimentation\n",
      "================================================================================\n",
      "\n",
      "1. SETTING UP MLFLOW AND CONFIGURATION\n",
      "----------------------------------------\n",
      "MLflow Configuration:\n",
      "  Tracking URI: file:./mlruns\n",
      "  Experiment: credit-risk-modeling\n",
      "\n",
      "2. LOADING AND PREPARING DATA\n",
      "----------------------------------------\n",
      "Loading data from data/processed/customers_with_proxy_target_fixed.csv...\n",
      "‚úì Loaded 3742 customer records\n",
      "Features: 10 features + CustomerId + Target\n",
      "\n",
      "Feature matrix shape: (3742, 10)\n",
      "Target distribution:\n",
      "is_high_risk\n",
      "1    83.5%\n",
      "0    16.5%\n",
      "Name: proportion, dtype: object\n",
      "‚úì No missing values found\n",
      "\n",
      "3. CREATING TRAIN-TEST SPLIT\n",
      "----------------------------------------\n",
      "Training set: 2993 samples (80.0%)\n",
      "Test set: 749 samples (20.0%)\n",
      "\n",
      "Class distribution in training set:\n",
      "is_high_risk\n",
      "1    83.5%\n",
      "0    16.5%\n",
      "Name: proportion, dtype: object\n",
      "\n",
      "Class distribution in test set:\n",
      "is_high_risk\n",
      "1    83.6%\n",
      "0    16.4%\n",
      "Name: proportion, dtype: object\n",
      "‚úì Saved train/test splits to data/processed/task5/\n",
      "\n",
      "4. DEFINING MODELS AND HYPERPARAMETERS\n",
      "----------------------------------------\n",
      "‚úì Saved scaler to models/task5/scaler.pkl\n",
      "Defined 4 models for training:\n",
      "  ‚Ä¢ LogisticRegression\n",
      "  ‚Ä¢ RandomForest\n",
      "  ‚Ä¢ GradientBoosting\n",
      "  ‚Ä¢ DecisionTree\n",
      "\n",
      "================================================================================\n",
      "5. TRAINING MODELS WITH MLFLOW TRACKING\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING: LogisticRegression\n",
      "============================================================\n",
      "  Performing GridSearchCV...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:41:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚úÖ Training complete for LogisticRegression\n",
      "     Best CV Score: 0.991\n",
      "     Test Accuracy: 0.965\n",
      "     Test ROC-AUC: 0.989\n",
      "     Best parameters: {'C': 100, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "============================================================\n",
      "TRAINING: RandomForest\n",
      "============================================================\n",
      "  Performing GridSearchCV...\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:43:27 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚úÖ Training complete for RandomForest\n",
      "     Best CV Score: 1.000\n",
      "     Test Accuracy: 0.995\n",
      "     Test ROC-AUC: 1.000\n",
      "     Best parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\n",
      "============================================================\n",
      "TRAINING: GradientBoosting\n",
      "============================================================\n",
      "  Performing GridSearchCV...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:44:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚úÖ Training complete for GradientBoosting\n",
      "     Best CV Score: 1.000\n",
      "     Test Accuracy: 0.997\n",
      "     Test ROC-AUC: 1.000\n",
      "     Best parameters: {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.8}\n",
      "\n",
      "============================================================\n",
      "TRAINING: DecisionTree\n",
      "============================================================\n",
      "  Performing GridSearchCV...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:44:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  ‚úÖ Training complete for DecisionTree\n",
      "     Best CV Score: 0.999\n",
      "     Test Accuracy: 0.995\n",
      "     Test ROC-AUC: 0.996\n",
      "     Best parameters: {'max_depth': 10, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "\n",
      "================================================================================\n",
      "6. MODEL COMPARISON AND SELECTION\n",
      "================================================================================\n",
      "\n",
      "Model Performance Comparison:\n",
      "        model_name  accuracy  precision   recall  f1_score  roc_auc\n",
      "      RandomForest  0.994660   0.998397 0.995208  0.996800 0.999818\n",
      "  GradientBoosting  0.997330   0.996815 1.000000  0.998405 0.999649\n",
      "      DecisionTree  0.994660   0.998397 0.995208  0.996800 0.995857\n",
      "LogisticRegression  0.965287   0.996689 0.961661  0.978862 0.989117\n",
      "\n",
      "üéØ BEST MODEL: RandomForest\n",
      "   ROC-AUC: 1.000\n",
      "   Accuracy: 0.995\n",
      "   F1-Score: 0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/15 18:45:03 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Successfully registered model 'CreditRiskModel'.\n",
      "2025/12/15 18:45:11 WARNING mlflow.tracking._model_registry.fluent: Run with id aea73f0a6acf4a73990373341327014f has no artifacts at artifact path 'best_model', registering model based on models:/m-e91d9f5efa454755a48a4a56bf51516d instead\n",
      "Created version '1' of model 'CreditRiskModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Registered RandomForest as 'CreditRiskModel' in MLflow Model Registry\n",
      "\n",
      "================================================================================\n",
      "7. DETAILED EVALUATION OF BEST MODEL: RandomForest\n",
      "================================================================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Low Risk       0.98      0.99      0.98       123\n",
      "   High Risk       1.00      1.00      1.00       626\n",
      "\n",
      "    accuracy                           0.99       749\n",
      "   macro avg       0.99      0.99      0.99       749\n",
      "weighted avg       0.99      0.99      0.99       749\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "                Low  High\n",
      "Actual Low     122     1\n",
      "Actual High      3   623\n",
      "‚úì Saved ROC curve to reports/roc_curve_best_model.png\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "              feature  importance\n",
      "            amount_cv    0.391424\n",
      "    transaction_count    0.220826\n",
      "high_fraud_hour_ratio    0.145905\n",
      " customer_tenure_days    0.085075\n",
      "           amount_std    0.052970\n",
      "         total_amount    0.033979\n",
      "           avg_amount    0.033463\n",
      "    frequency_per_day    0.022743\n",
      "high_risk_channel_use    0.010149\n",
      "         recency_days    0.003467\n",
      "‚úì Saved feature importance plot to reports/top_features_best_model.png\n",
      "\n",
      "================================================================================\n",
      "8. SAVING FINAL RESULTS AND ARTIFACTS\n",
      "================================================================================\n",
      "‚úì Saved best model to models/task5/best_model.pkl\n",
      "‚úì Saved model comparison results to reports/model_comparison_results.csv\n",
      "‚úì Saved metrics summary to reports/metrics_summary.json\n",
      "‚úì Saved sample predictions to reports/sample_predictions.csv\n",
      "\n",
      "================================================================================\n",
      "9. CREATING UNIT TESTS\n",
      "================================================================================\n",
      "‚úì Created unit tests at tests/test_model_training.py\n",
      "  To run tests: python -m pytest tests/test_model_training.py\n",
      "\n",
      "================================================================================\n",
      "TASK 5 COMPLETE - SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "‚úÖ ALL TASK REQUIREMENTS SATISFIED:\n",
      "   Setup MLflow and pytest                       ‚úì\n",
      "   Data preparation and train-test split         ‚úì\n",
      "   Train at least 2 models                       ‚úì (4 models trained)\n",
      "   Hyperparameter tuning with GridSearchCV       ‚úì\n",
      "   Experiment tracking with MLflow               ‚úì\n",
      "   Model evaluation with metrics                 ‚úì\n",
      "   Unit tests created                            ‚úì\n",
      "   Best model registered in MLflow               ‚úì\n",
      "\n",
      "üìä MODEL PERFORMANCE SUMMARY:\n",
      "   Best Model: RandomForest\n",
      "   ROC-AUC Score: 1.000\n",
      "   Accuracy: 0.995\n",
      "   F1-Score: 0.997\n",
      "\n",
      "üîß MODELS TRAINED:\n",
      "   ‚Ä¢ LogisticRegression   ROC-AUC: 0.989\n",
      "   ‚Ä¢ RandomForest         ROC-AUC: 1.000\n",
      "   ‚Ä¢ GradientBoosting     ROC-AUC: 1.000\n",
      "   ‚Ä¢ DecisionTree         ROC-AUC: 0.996\n",
      "\n",
      "üíæ ARTIFACTS CREATED:\n",
      "   ‚úì models/task5/\n",
      "     Trained models and scaler\n",
      "   ‚úì data/processed/task5/\n",
      "     Train/test splits\n",
      "   ‚úì reports/\n",
      "     Evaluation reports and visualizations\n",
      "   ‚úì mlruns/\n",
      "     MLflow experiment tracking\n",
      "   ‚úì tests/test_model_training.py\n",
      "     Unit tests\n",
      "\n",
      "üìà NEXT STEPS FOR TASK 6 (DEPLOYMENT):\n",
      "   1. Create FastAPI application (src/api/main.py)\n",
      "   2. Build Pydantic models for request/response\n",
      "   3. Create Dockerfile for containerization\n",
      "   4. Set up CI/CD pipeline (.github/workflows/ci.yml)\n",
      "   5. Test API endpoints\n",
      "\n",
      "üîç TO VIEW MLFLOW EXPERIMENTS:\n",
      "   $ mlflow ui --port 5000\n",
      "   Then open: http://localhost:5000\n",
      "\n",
      "üß™ TO RUN UNIT TESTS:\n",
      "   $ python -m pytest tests/test_model_training.py -v\n",
      "\n",
      "================================================================================\n",
      "READY FOR TASK 6 - MODEL DEPLOYMENT\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Task 5 - Model Training and Tracking\n",
    "Complete ML pipeline with MLflow tracking\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Import MLflow and sklearn components\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix, \n",
    "    classification_report, roc_curve, auc\n",
    ")\n",
    "\n",
    "# Import models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Import for saving\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TASK 5 - MODEL TRAINING AND TRACKING\")\n",
    "print(\"Credit Risk Model with MLflow Experimentation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Setup and Configuration\n",
    "# -----------------------------\n",
    "print(\"\\n1. SETTING UP MLFLOW AND CONFIGURATION\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Set MLflow tracking URI (local directory)\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"credit-risk-modeling\")\n",
    "\n",
    "print(\"MLflow Configuration:\")\n",
    "print(f\"  Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"  Experiment: credit-risk-modeling\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('models/task5', exist_ok=True)\n",
    "os.makedirs('reports', exist_ok=True)\n",
    "os.makedirs('data/processed/task5', exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load and Prepare Data\n",
    "# -----------------------------\n",
    "print(\"\\n2. LOADING AND PREPARING DATA\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Load the data from Task 4\n",
    "data_path = \"data/processed/customers_with_proxy_target_fixed.csv\"\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"‚ùå Error: Data file not found at {data_path}\")\n",
    "    print(\"Please run Task 4 first.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"Loading data from {data_path}...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"‚úì Loaded {len(df)} customer records\")\n",
    "print(f\"Features: {df.shape[1] - 2} features + CustomerId + Target\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(['CustomerId', 'is_high_risk'], axis=1)\n",
    "y = df['is_high_risk']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(y.value_counts(normalize=True).apply(lambda x: f\"{x:.1%}\"))\n",
    "\n",
    "# Check for any missing values\n",
    "missing = X.isnull().sum().sum()\n",
    "if missing > 0:\n",
    "    print(f\"‚ö† Found {missing} missing values. Imputing with median...\")\n",
    "    X = X.fillna(X.median())\n",
    "else:\n",
    "    print(\"‚úì No missing values found\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Train-Test Split\n",
    "# -----------------------------\n",
    "print(\"\\n3. CREATING TRAIN-TEST SPLIT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True).apply(lambda x: f\"{x:.1%}\"))\n",
    "\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).apply(lambda x: f\"{x:.1%}\"))\n",
    "\n",
    "# Save the splits\n",
    "X_train.to_csv('data/processed/task5/X_train.csv', index=False)\n",
    "X_test.to_csv('data/processed/task5/X_test.csv', index=False)\n",
    "pd.Series(y_train).to_csv('data/processed/task5/y_train.csv', index=False, header=['is_high_risk'])\n",
    "pd.Series(y_test).to_csv('data/processed/task5/y_test.csv', index=False, header=['is_high_risk'])\n",
    "\n",
    "print(\"‚úì Saved train/test splits to data/processed/task5/\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Define Models and Hyperparameters\n",
    "# -----------------------------\n",
    "print(\"\\n4. DEFINING MODELS AND HYPERPARAMETERS\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'models/task5/scaler.pkl')\n",
    "print(\"‚úì Saved scaler to models/task5/scaler.pkl\")\n",
    "\n",
    "# Define models with hyperparameter grids\n",
    "models = {\n",
    "    \"LogisticRegression\": {\n",
    "        \"model\": LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "        \"params\": {\n",
    "            'C': [0.01, 0.1, 1, 10, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'max_iter': [1000]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"model\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"DecisionTree\": {\n",
    "        \"model\": DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "        \"params\": {\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Defined {len(models)} models for training:\")\n",
    "for model_name in models.keys():\n",
    "    print(f\"  ‚Ä¢ {model_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Training with MLflow Tracking\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. TRAINING MODELS WITH MLFLOW TRACKING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_models = {}\n",
    "all_results = []\n",
    "\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=model_name):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"model\", model_name)\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        \n",
    "        # Perform grid search\n",
    "        print(f\"  Performing GridSearchCV...\")\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_config[\"model\"],\n",
    "            param_grid=model_config[\"params\"],\n",
    "            cv=5,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get best model\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_models[model_name] = best_model\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = best_model.predict(X_test_scaled)\n",
    "        y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"roc_auc\", roc_auc)\n",
    "        mlflow.log_metric(\"best_cv_score\", grid_search.best_score_)\n",
    "        \n",
    "        # Log parameters\n",
    "        for param, value in grid_search.best_params_.items():\n",
    "            mlflow.log_param(f\"best_{param}\", value)\n",
    "        \n",
    "        # Save results\n",
    "        model_results = {\n",
    "            \"model_name\": model_name,\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1),\n",
    "            \"roc_auc\": float(roc_auc),\n",
    "            \"best_cv_score\": float(grid_search.best_score_),\n",
    "            \"best_params\": grid_search.best_params_,\n",
    "            \"feature_importance\": None\n",
    "        }\n",
    "        \n",
    "        # Log feature importance if available\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            feature_importance = dict(zip(X.columns, best_model.feature_importances_))\n",
    "            model_results[\"feature_importance\"] = feature_importance\n",
    "            \n",
    "            # Create feature importance plot\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sorted_idx = np.argsort(best_model.feature_importances_)[::-1]\n",
    "            plt.bar(range(len(sorted_idx)), best_model.feature_importances_[sorted_idx])\n",
    "            plt.xticks(range(len(sorted_idx)), X.columns[sorted_idx], rotation=45, ha='right')\n",
    "            plt.title(f'{model_name} - Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            importance_path = f'reports/feature_importance_{model_name}.png'\n",
    "            plt.savefig(importance_path)\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(importance_path)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(best_model, f\"model_{model_name}\")\n",
    "        \n",
    "        # Save model locally\n",
    "        model_path = f'models/task5/{model_name}_best.pkl'\n",
    "        joblib.dump(best_model, model_path)\n",
    "        mlflow.log_artifact(model_path)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        cm_df = pd.DataFrame(cm, \n",
    "                           index=['Actual Low Risk', 'Actual High Risk'],\n",
    "                           columns=['Predicted Low Risk', 'Predicted High Risk'])\n",
    "        \n",
    "        cm_path = f'reports/confusion_matrix_{model_name}.csv'\n",
    "        cm_df.to_csv(cm_path)\n",
    "        mlflow.log_artifact(cm_path)\n",
    "        \n",
    "        all_results.append(model_results)\n",
    "        \n",
    "        print(f\"\\n  ‚úÖ Training complete for {model_name}\")\n",
    "        print(f\"     Best CV Score: {grid_search.best_score_:.3f}\")\n",
    "        print(f\"     Test Accuracy: {accuracy:.3f}\")\n",
    "        print(f\"     Test ROC-AUC: {roc_auc:.3f}\")\n",
    "        print(f\"     Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Model Comparison and Selection\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"6. MODEL COMPARISON AND SELECTION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sort by ROC-AUC (primary metric for imbalanced data)\n",
    "results_df = results_df.sort_values('roc_auc', ascending=False)\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_df[['model_name', 'accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']].to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['model_name']\n",
    "best_model = best_models[best_model_name]\n",
    "best_model_metrics = results_df.iloc[0].to_dict()\n",
    "\n",
    "print(f\"\\nüéØ BEST MODEL: {best_model_name}\")\n",
    "print(f\"   ROC-AUC: {best_model_metrics['roc_auc']:.3f}\")\n",
    "print(f\"   Accuracy: {best_model_metrics['accuracy']:.3f}\")\n",
    "print(f\"   F1-Score: {best_model_metrics['f1_score']:.3f}\")\n",
    "\n",
    "# Register best model in MLflow\n",
    "with mlflow.start_run(run_name=\"Best_Model_Registration\"):\n",
    "    mlflow.log_params(best_model_metrics['best_params'])\n",
    "    for metric, value in best_model_metrics.items():\n",
    "        if metric not in ['model_name', 'best_params', 'feature_importance']:\n",
    "            mlflow.log_metric(metric, value)\n",
    "    \n",
    "    # Log the best model\n",
    "    mlflow.sklearn.log_model(best_model, \"best_model\")\n",
    "    \n",
    "    # Register the model\n",
    "    model_uri = f\"runs:/{mlflow.active_run().info.run_id}/best_model\"\n",
    "    mlflow.register_model(model_uri, \"CreditRiskModel\")\n",
    "    \n",
    "    print(f\"‚úì Registered {best_model_name} as 'CreditRiskModel' in MLflow Model Registry\")\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Detailed Evaluation of Best Model\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"7. DETAILED EVALUATION OF BEST MODEL: {best_model_name}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Make predictions with best model\n",
    "y_pred_best = best_model.predict(X_test_scaled)\n",
    "y_pred_proba_best = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, \n",
    "                          target_names=['Low Risk', 'High Risk']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                Low  High\")\n",
    "print(f\"Actual Low   {cm[0,0]:5d} {cm[0,1]:5d}\")\n",
    "print(f\"Actual High  {cm[1,0]:5d} {cm[1,1]:5d}\")\n",
    "\n",
    "# ROC Curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_best)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "         label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(f'ROC Curve - {best_model_name}')\n",
    "plt.legend(loc=\"lower right\")\n",
    "roc_path = 'reports/roc_curve_best_model.png'\n",
    "plt.savefig(roc_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f\"‚úì Saved ROC curve to {roc_path}\")\n",
    "\n",
    "# Feature importance for tree-based models\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': best_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = feature_importance.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'{best_model_name} - Top 10 Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    feature_plot_path = 'reports/top_features_best_model.png'\n",
    "    plt.savefig(feature_plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"‚úì Saved feature importance plot to {feature_plot_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 8. Save Final Results\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"8. SAVING FINAL RESULTS AND ARTIFACTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Save best model\n",
    "best_model_path = 'models/task5/best_model.pkl'\n",
    "joblib.dump(best_model, best_model_path)\n",
    "print(f\"‚úì Saved best model to {best_model_path}\")\n",
    "\n",
    "# Save results summary\n",
    "results_path = 'reports/model_comparison_results.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"‚úì Saved model comparison results to {results_path}\")\n",
    "\n",
    "# Save metrics summary\n",
    "metrics_summary = {\n",
    "    'best_model': best_model_name,\n",
    "    'best_model_metrics': best_model_metrics,\n",
    "    'training_timestamp': datetime.now().isoformat(),\n",
    "    'data_summary': {\n",
    "        'total_customers': len(df),\n",
    "        'train_samples': len(X_train),\n",
    "        'test_samples': len(X_test),\n",
    "        'feature_count': X.shape[1],\n",
    "        'class_distribution': y.value_counts().to_dict()\n",
    "    },\n",
    "    'model_performance': results_df.to_dict('records')\n",
    "}\n",
    "\n",
    "metrics_path = 'reports/metrics_summary.json'\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2, default=str)\n",
    "print(f\"‚úì Saved metrics summary to {metrics_path}\")\n",
    "\n",
    "# Save prediction examples\n",
    "predictions_df = pd.DataFrame({\n",
    "    'customer_id': df.loc[X_test.index, 'CustomerId'].values,\n",
    "    'actual_risk': y_test.values,\n",
    "    'predicted_risk': y_pred_best,\n",
    "    'risk_probability': y_pred_proba_best\n",
    "})\n",
    "predictions_path = 'reports/sample_predictions.csv'\n",
    "predictions_df.head(100).to_csv(predictions_path, index=False)\n",
    "print(f\"‚úì Saved sample predictions to {predictions_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 9. Create Unit Tests\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"9. CREATING UNIT TESTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create test directory if not exists\n",
    "os.makedirs('tests', exist_ok=True)\n",
    "\n",
    "# Create a simple unit test file\n",
    "test_code = '''\n",
    "\"\"\"\n",
    "Unit tests for Task 5 - Model Training\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "class TestModelTraining(unittest.TestCase):\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Load test data and model\"\"\"\n",
    "        self.model_path = 'models/task5/best_model.pkl'\n",
    "        self.data_path = 'data/processed/task5/X_test.csv'\n",
    "        self.target_path = 'data/processed/task5/y_test.csv'\n",
    "    \n",
    "    def test_model_exists(self):\n",
    "        \"\"\"Test that the best model file exists\"\"\"\n",
    "        self.assertTrue(os.path.exists(self.model_path), \n",
    "                       f\"Model file not found at {self.model_path}\")\n",
    "    \n",
    "    def test_data_exists(self):\n",
    "        \"\"\"Test that test data exists\"\"\"\n",
    "        self.assertTrue(os.path.exists(self.data_path), \n",
    "                       f\"Test data not found at {self.data_path}\")\n",
    "        self.assertTrue(os.path.exists(self.target_path), \n",
    "                       f\"Test target not found at {self.target_path}\")\n",
    "    \n",
    "    def test_model_loading(self):\n",
    "        \"\"\"Test that the model can be loaded\"\"\"\n",
    "        if os.path.exists(self.model_path):\n",
    "            model = joblib.load(self.model_path)\n",
    "            self.assertIsNotNone(model, \"Failed to load model\")\n",
    "    \n",
    "    def test_prediction_shape(self):\n",
    "        \"\"\"Test that predictions have correct shape\"\"\"\n",
    "        if os.path.exists(self.model_path) and os.path.exists(self.data_path):\n",
    "            model = joblib.load(self.model_path)\n",
    "            X_test = pd.read_csv(self.data_path)\n",
    "            predictions = model.predict(X_test)\n",
    "            self.assertEqual(len(predictions), len(X_test), \n",
    "                           \"Predictions shape doesn't match test data\")\n",
    "    \n",
    "    def test_feature_count(self):\n",
    "        \"\"\"Test that feature count is consistent\"\"\"\n",
    "        if os.path.exists(self.data_path):\n",
    "            X_test = pd.read_csv(self.data_path)\n",
    "            # Assuming model expects 10 features based on Task 4\n",
    "            self.assertEqual(X_test.shape[1], 10, \n",
    "                           f\"Expected 10 features, got {X_test.shape[1]}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "'''\n",
    "\n",
    "test_file_path = 'tests/test_model_training.py'\n",
    "with open(test_file_path, 'w') as f:\n",
    "    f.write(test_code)\n",
    "\n",
    "print(f\"‚úì Created unit tests at {test_file_path}\")\n",
    "print(\"  To run tests: python -m pytest tests/test_model_training.py\")\n",
    "\n",
    "# -----------------------------\n",
    "# 10. Summary Report\n",
    "# -----------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TASK 5 COMPLETE - SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n‚úÖ ALL TASK REQUIREMENTS SATISFIED:\")\n",
    "requirements = [\n",
    "    (\"Setup MLflow and pytest\", \"‚úì\"),\n",
    "    (\"Data preparation and train-test split\", \"‚úì\"),\n",
    "    (\"Train at least 2 models\", f\"‚úì ({len(models)} models trained)\"),\n",
    "    (\"Hyperparameter tuning with GridSearchCV\", \"‚úì\"),\n",
    "    (\"Experiment tracking with MLflow\", \"‚úì\"),\n",
    "    (\"Model evaluation with metrics\", \"‚úì\"),\n",
    "    (\"Unit tests created\", \"‚úì\"),\n",
    "    (\"Best model registered in MLflow\", \"‚úì\")\n",
    "]\n",
    "\n",
    "for req, status in requirements:\n",
    "    print(f\"   {req:45} {status}\")\n",
    "\n",
    "print(f\"\\nüìä MODEL PERFORMANCE SUMMARY:\")\n",
    "print(f\"   Best Model: {best_model_name}\")\n",
    "print(f\"   ROC-AUC Score: {best_model_metrics['roc_auc']:.3f}\")\n",
    "print(f\"   Accuracy: {best_model_metrics['accuracy']:.3f}\")\n",
    "print(f\"   F1-Score: {best_model_metrics['f1_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nüîß MODELS TRAINED:\")\n",
    "for model_name in models.keys():\n",
    "    result = results_df[results_df['model_name'] == model_name].iloc[0]\n",
    "    print(f\"   ‚Ä¢ {model_name:20} ROC-AUC: {result['roc_auc']:.3f}\")\n",
    "\n",
    "print(f\"\\nüíæ ARTIFACTS CREATED:\")\n",
    "artifacts = [\n",
    "    ('models/task5/', 'Trained models and scaler'),\n",
    "    ('data/processed/task5/', 'Train/test splits'),\n",
    "    ('reports/', 'Evaluation reports and visualizations'),\n",
    "    ('mlruns/', 'MLflow experiment tracking'),\n",
    "    ('tests/test_model_training.py', 'Unit tests')\n",
    "]\n",
    "\n",
    "for path, desc in artifacts:\n",
    "    if os.path.exists(path if not path.endswith('.py') else os.path.dirname(path)):\n",
    "        print(f\"   ‚úì {path}\")\n",
    "        print(f\"     {desc}\")\n",
    "\n",
    "print(f\"\\nüìà NEXT STEPS FOR TASK 6 (DEPLOYMENT):\")\n",
    "print(f\"   1. Create FastAPI application (src/api/main.py)\")\n",
    "print(f\"   2. Build Pydantic models for request/response\")\n",
    "print(f\"   3. Create Dockerfile for containerization\")\n",
    "print(f\"   4. Set up CI/CD pipeline (.github/workflows/ci.yml)\")\n",
    "print(f\"   5. Test API endpoints\")\n",
    "\n",
    "print(f\"\\nüîç TO VIEW MLFLOW EXPERIMENTS:\")\n",
    "print(f\"   $ mlflow ui --port 5000\")\n",
    "print(f\"   Then open: http://localhost:5000\")\n",
    "\n",
    "print(f\"\\nüß™ TO RUN UNIT TESTS:\")\n",
    "print(f\"   $ python -m pytest tests/test_model_training.py -v\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"READY FOR TASK 6 - MODEL DEPLOYMENT\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61fa6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
